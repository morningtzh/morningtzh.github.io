<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>回归 | 瑟瑟和你说早安</title>
    <link>https://blog.morningtzh.com/tags/%E5%9B%9E%E5%BD%92/</link>
      <atom:link href="https://blog.morningtzh.com/tags/%E5%9B%9E%E5%BD%92/index.xml" rel="self" type="application/rss+xml" />
    <description>回归</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>zh-Hans</language><copyright>©MorningTZH</copyright><lastBuildDate>Sun, 01 Dec 2019 09:59:04 +0800</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>回归</title>
      <link>https://blog.morningtzh.com/tags/%E5%9B%9E%E5%BD%92/</link>
    </image>
    
    <item>
      <title>线性回归及实现</title>
      <link>https://blog.morningtzh.com/post/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8F%8A%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Sun, 01 Dec 2019 09:59:04 +0800</pubDate>
      <guid>https://blog.morningtzh.com/post/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%8F%8A%E5%AE%9E%E7%8E%B0/</guid>
      <description>&lt;h1 id=&#34;线性回归及实现&#34;&gt;线性回归及实现&lt;/h1&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;一年多前，《修真聊天群》更新看完了，在书荒引发的无聊情绪推动下学习了 Keras，用 LSTM 生成了大段无法看的《伪*修真聊天群》，没有耐心好好炼丹就没有再接触过深度学习的知识点，偶尔无聊看看聚宽上的文章也都是&lt;code&gt;随机森林&lt;/code&gt;和&lt;code&gt;隐马尔科夫模型&lt;/code&gt;等传统机器学习算法。
近期 gayhub 上榜了一个
&lt;a href=&#34;https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《TensorFlow 2.0深度学习开源书》&lt;/a&gt;，正巧之后的新工作也与 TensorFlow 有些许关联，就趁机学习一下。&lt;/p&gt;
&lt;p&gt;看了教程打算手撸下公式推导及函数实现。（ &amp;lt;&amp;ndash; 抄袭狗 ）&lt;/p&gt;
&lt;h2 id=&#34;1线性回归&#34;&gt;1.线性回归&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w&amp;rsquo;x+e，e为误差服从均值为0的正态分布。 &amp;mdash;- 百度百科&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;11-二元一次方程&#34;&gt;1.1 二元一次方程&lt;/h3&gt;
&lt;p&gt;当我们有个函数为 $y=wx+b$ 并且知道本函数穿过的两个点时我们可以轻易的算出函数中的常数$w$和$b$。&lt;/p&gt;
&lt;p&gt;假设有两点：
$$
x_1=3,y_1=7 \&lt;br&gt;
x_2=4,y_2=6 \&lt;br&gt;
$$&lt;/p&gt;
&lt;p&gt;则其解为：
$$
7=3w+b \&lt;br&gt;
6=4w+b \&lt;br&gt;
w=-1,b=10
$$&lt;/p&gt;
&lt;p&gt;其曲线为：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
plt.plot([3,4],[7,6])
plt.show() # show figure
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;12-观测误差&#34;&gt;1.2 观测误差&lt;/h3&gt;
&lt;p&gt;但是当我们的观测点存在误差时，公式变为了 $y=wx+b+\epsilon,\epsilon\in\mathcal{N}(\mu,\sigma^2)$. 此时我们无法用过之前的解法解出$w$和$b$的值。代码中采用&lt;code&gt;random.random()-0.5&lt;/code&gt;模拟$\epsilon$，我们会发现无论取哪两点去计算$w$和$b$都会导致函数存在巨大的偏离。我们只能通过回归分析去计算最佳的$w$和$b$。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
import random
x=range(0, 10)
plt.plot(x,[-1*i+10+random.random()-0.5 for i in x], &#39;o&#39;)
plt.plot(x,[-1*i+10 for i in x])
plt.show() # show figure
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;13-损失函数loss&#34;&gt;1.3 损失函数（loss）&lt;/h3&gt;
&lt;p&gt;此时我们引入了损失函数来计算$w$和$b$的好坏，在此例中采用均方误差(MSE)来评判：
$$
\mathscr{L}=\frac{\sum_{i=1}^{n}(wx_i+b-y_i)^2}n
$$&lt;/p&gt;
&lt;p&gt;我们接下来的工作就是找到使$\mathscr{L}$最小的$w$和$b$。&lt;/p&gt;
&lt;h3 id=&#34;14-梯度下降及优化函数&#34;&gt;1.4 梯度下降及优化函数&lt;/h3&gt;
&lt;p&gt;我们通过寻找$\mathscr{L}$最小值来确定最佳的$w$和$b$，此时可通过求极值的方式来寻找损失函数的极小值。损失函数为三元方程，其偏导数即为梯度，我们用&lt;code&gt;学习率(lr)&lt;/code&gt;来缩放梯度，通过不断的减去梯度来计算$w$和$b$，将其优化到最佳。
$$
w&#39;=w-\eta\frac{\partial \mathscr{L}}{\partial w} \&lt;br&gt;
b&#39;=b-\eta\frac{\partial \mathscr{L}}{\partial b} \&lt;br&gt;
\eta\text{为学习率 lr}
$$&lt;/p&gt;
&lt;p&gt;解开偏导数以备后用：
$$
\begin{align}
\frac{\partial \mathscr{L}}{\partial w} &amp;amp; = \frac{\partial \frac{\sum_{i=1}^{n}(wx_i+b-y_i)^2}n}{\partial w} \&lt;br&gt;
&amp;amp; = \frac{2}{n}\sum_{i=1}^{n} (wx_i+b-y_i)\frac{\partial (wx_i+b-y_i)}{\partial w} \&lt;br&gt;
&amp;amp; = \frac{2}{n}\sum_{i=1}^{n} (wx_i+b-y_i)x_i
\end{align}
$$
$$
\begin{align}
\frac{\partial \mathscr{L}}{\partial b} &amp;amp; = \frac{\partial \frac{\sum_{i=1}^{n}(bx_i+b-y_i)^2}n}{\partial b} \&lt;br&gt;
&amp;amp; = \frac{2}{n}\sum_{i=1}^{n} (bx_i+b-y_i)\frac{\partial (bx_i+b-y_i)}{\partial b} \&lt;br&gt;
&amp;amp; = \frac{2}{n}\sum_{i=1}^{n} (bx_i+b-y_i)
\end{align}
$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
